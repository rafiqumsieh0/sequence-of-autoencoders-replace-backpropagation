{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sequence_of_autoencoders_mnist.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1U9SBBLapSsj"
      },
      "source": [
        "**Imports**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9Wyup8IUtWM"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as kb\n",
        "import numpy as np\n",
        "import random\n",
        "from tensorflow.keras import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, Conv1D, MaxPooling2D, MaxPooling1D, BatchNormalization, Input, Reshape, Dropout, ActivityRegularization, Activation\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from scipy.spatial import distance\n",
        "from scipy.special import softmax\n",
        "from tensorflow.keras import regularizers\n",
        "from google.colab.patches import cv2_imshow\n",
        "import time\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.cm as cm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wO3t9doLsx-M"
      },
      "source": [
        "**Declaration of the Experiment class which includes methods for creating the neural networks, retrieving and pre-processing the dataset, and starting the experiment**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqzJHv04xkt_"
      },
      "source": [
        "class NegativeWeightsConstraint(tf.keras.constraints.Constraint):\n",
        "\n",
        "  def map_pos_to_zero(element):\n",
        "    if element > 0.0:\n",
        "      return 0.0\n",
        "    else:\n",
        "      return element\n",
        "\n",
        "  def __call__(self, w):\n",
        "    return tf.map_fn(self.map_pos_to_zero, w)\n",
        "\n",
        "class Experiment:\n",
        "\n",
        "\n",
        "  def __init__(self, learning_rate, learning_rate_trad, learning_rate_preconv, num_classes, num_train_images_per_digit, num_test_images_per_digit, num_epochs, encoder_sizes):\n",
        "    self.dataset = self.load_dataset()\n",
        "    self.learning_rate = learning_rate\n",
        "    self.learning_rate_trad = learning_rate_trad\n",
        "    self.learning_rate_preconv = learning_rate_preconv\n",
        "    self.num_classes = num_classes\n",
        "    self.num_train_images_per_digit = num_train_images_per_digit\n",
        "    self.num_test_images_per_digit = num_test_images_per_digit\n",
        "    self.num_epochs = num_epochs\n",
        "    self.autoencoders = []\n",
        "    self.intermediates = []\n",
        "    self.encoder_sizes = encoder_sizes\n",
        "    self.outputs = [[] for _ in range(self.num_classes)]\n",
        "    self.averages = [[0.0]*self.encoder_sizes[-1] for _ in range(self.num_classes)]\n",
        "    self.sums = [[0.0]*self.encoder_sizes[-1] for _ in range(self.num_classes)]\n",
        "    self.std_devs = [[0.0]*self.encoder_sizes[-1] for _ in range(self.num_classes)]\n",
        "    \n",
        "  def custom_loss(self, y_actual, y_pred):\n",
        "    #custom_loss = tf.math.square(y_actual-y_pred) + tf.losses.cosine_similarity(y_actual, y_pred) \n",
        "    custom_loss = kb.sqrt(kb.sum(kb.square(y_pred - y_actual), axis=-1))\n",
        "    return custom_loss\n",
        "\n",
        "  class PoolingKernelInitializer(tf.keras.initializers.Initializer):  \n",
        "    def __call__(self, shape, dtype=None):\n",
        "      first_layer_size = shape[0]\n",
        "      second_layer_size = shape[1]\n",
        "      weight_matrix = [[0.0 for _ in range(second_layer_size)] for _ in range(first_layer_size)]\n",
        "      sqrt_first_layer = math.floor(pow(first_layer_size, 0.5))\n",
        "      sqrt_second_layer = math.floor(pow(second_layer_size, 0.5))\n",
        "      ratio = math.floor(sqrt_first_layer/sqrt_second_layer)\n",
        "      for i in range(0, first_layer_size):\n",
        "          row1 = math.floor(i/sqrt_first_layer)\n",
        "          column1 = i % sqrt_first_layer\n",
        "          for j in range(0, second_layer_size):\n",
        "              row2 = math.floor(j/sqrt_second_layer)\n",
        "              column2 = j % sqrt_second_layer\n",
        "              if abs(math.floor(row1 / ratio) - row2) < 1 and abs(math.floor(column1 / ratio) - column2) < 1:\n",
        "                  #weight_matrix[i][j] = np.random.choice([-0.25, 0.75], p=[1 / 4, 3 / 4])\n",
        "                  weight_matrix[i][j] = np.random.normal()\n",
        "              else:\n",
        "                  weight_matrix[i][j] = 0.0\n",
        "      return tf.convert_to_tensor(np.array(weight_matrix), dtype=tf.float32)\n",
        "\n",
        "  class InversePoolingKernelInitializer(tf.keras.initializers.Initializer):  \n",
        "    def __call__(self, shape, dtype=None):\n",
        "      first_layer_size = shape[0]\n",
        "      second_layer_size = shape[1]\n",
        "      weight_matrix = [[0.0 for _ in range(second_layer_size)] for _ in range(first_layer_size)]\n",
        "      sqrt_first_layer = math.floor(pow(first_layer_size, 0.5))\n",
        "      sqrt_second_layer = math.floor(pow(second_layer_size, 0.5))\n",
        "      ratio = math.floor(sqrt_second_layer/sqrt_first_layer)\n",
        "      for i in range(0, first_layer_size):\n",
        "          row1 = math.floor(i/sqrt_first_layer)\n",
        "          column1 = i % sqrt_first_layer\n",
        "          for j in range(0, second_layer_size):\n",
        "              row2 = math.floor(j/sqrt_second_layer)\n",
        "              column2 = j % sqrt_second_layer\n",
        "              if abs(math.floor(row2 / ratio) - row1) < 1 and abs(math.floor(column2 / ratio) - column1) < 1:\n",
        "                  #weight_matrix[i][j] = np.random.choice([-0.25, 0.75], p=[1 / 4, 3 / 4])\n",
        "                  weight_matrix[i][j] = np.random.normal()\n",
        "              else:\n",
        "                  weight_matrix[i][j] = 0.0\n",
        "      #return tf.constant(np.array(weight_matrix))\n",
        "      return tf.convert_to_tensor(np.array(weight_matrix), dtype=tf.float32)\n",
        "\n",
        "\n",
        "  def shallow_autoencoder_model(self, encoder_size, encoder_index):\n",
        "    neural_net = Sequential()\n",
        "    neural_net.add(Flatten(input_shape = (int(math.sqrt(encoder_size)), int(math.sqrt(encoder_size)), 1)))\n",
        "    # neural_net.add(Dense(round(self.encoder_sizes[encoder_index + 1]),\n",
        "    #                   activation='linear',\n",
        "    #                   bias_initializer=tf.constant_initializer(value=-0.001),\n",
        "    #                   kernel_initializer=self.PoolingKernelInitializer(),\n",
        "    #                   kernel_regularizer=tf.keras.regularizers.l1(l1=0.000),\n",
        "    #                   activity_regularizer=tf.keras.regularizers.l1(l1=0.005)))\n",
        "    # neural_net.add(Activation('selu'))\n",
        "    # neural_net.add(Dense(encoder_size,\n",
        "    #                       activation='linear',\n",
        "    #                       bias_initializer=tf.constant_initializer(value=0.000),\n",
        "    #                       kernel_initializer=self.InversePoolingKernelInitializer,\n",
        "    #                       kernel_regularizer=tf.keras.regularizers.l1(l1=0.000),\n",
        "    #                       activity_regularizer=tf.keras.regularizers.l1(l1=0.005)))\n",
        "    neural_net.add(Dense(round(self.encoder_sizes[encoder_index + 1]),\n",
        "                         activation='linear',\n",
        "                         activity_regularizer=tf.keras.regularizers.l1_l2(l1=0.01, l2=0.01),\n",
        "                         #kernel_regularizer=tf.keras.regularizers.l1(l1=0.000),\n",
        "                         #bias_initializer=tf.keras.initializers.constant(value=-1.0)\n",
        "                         ))\n",
        "    neural_net.add(Activation('selu'))\n",
        "    neural_net.add(Dense(encoder_size,\n",
        "                         activation='linear'))\n",
        "    neural_net.add(Activation('sigmoid'))\n",
        "    neural_net.add(Reshape((int(math.sqrt(encoder_size)), int(math.sqrt(encoder_size)), 1)))\n",
        "    neural_net.compile(optimizer=RMSprop(lr=self.learning_rate), loss='mse', metrics=['accuracy'])\n",
        "    return neural_net\n",
        "\n",
        "  def conv_encoder(self):\n",
        "    neural_net = Sequential()\n",
        "    neural_net.add(Conv2D(32, activation='relu', input_shape=(28, 28, 1), kernel_size=(3, 3), strides=(1, 1), padding='same'))\n",
        "    neural_net.add(BatchNormalization())\n",
        "    neural_net.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2))) \n",
        "    neural_net.add(Conv2D(16, activation='relu', input_shape=(28, 28, 1), kernel_size=(3, 3), strides=(1, 1), padding='same'))\n",
        "    neural_net.add(BatchNormalization())\n",
        "    neural_net.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2))) \n",
        "    neural_net.add(Flatten())\n",
        "    neural_net.add(Dense(196, activation='relu'))\n",
        "    neural_net.add(Dense(784, activation='relu'))\n",
        "    neural_net.add(Reshape((28, 28, 1)))\n",
        "    neural_net.compile(optimizer=Adam(lr=self.learning_rate_preconv), loss='mse', metrics=['accuracy'])\n",
        "    return neural_net\n",
        "\n",
        "  def conv_multi_classifier_model(self, learning_rate):\n",
        "\n",
        "    neural_net = Sequential()\n",
        "    \n",
        "    neural_net.add(Conv2D(64, activation='relu', input_shape=(28, 28, 1), kernel_size=(3, 3), strides=(1, 1), padding='same'))\n",
        "    neural_net.add(BatchNormalization())\n",
        "    neural_net.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "\n",
        "    neural_net.add(Flatten())\n",
        "\n",
        "    neural_net.add(Dense(128, activation='relu'))\n",
        "    neural_net.add(Dense(32, activation='relu'))\n",
        "    neural_net.add(Dense(10, activation='relu'))\n",
        "    neural_net.add(Dense(self.num_classes, activation='softmax'))\n",
        "\n",
        "    neural_net.compile(optimizer=Adam(lr=learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return neural_net\n",
        "\n",
        "\n",
        "  def ff_multi_classifier_model(self, learning_rate):\n",
        "    neural_net = Sequential()\n",
        "    neural_net.add(Flatten(input_shape = (28, 28)))\n",
        "    neural_net.add(Dense(128, activation='relu'))\n",
        "    neural_net.add(Dense(32, activation='relu'))\n",
        "    neural_net.add(Dense(10, activation='relu'))\n",
        "    neural_net.add(Dense(self.num_classes, activation='softmax'))\n",
        "    neural_net.compile(optimizer=Adam(lr=learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return neural_net\n",
        "\n",
        "\n",
        "  def load_dataset(self):\n",
        "    mnist_dataset = tf.keras.datasets.mnist.load_data()\n",
        "    (x_train, y_train) , (x_test, y_test) = mnist_dataset\n",
        "    x_train = x_train / 255.0\n",
        "    x_test = x_test / 255.0\n",
        "    return (x_train, y_train) , (x_test, y_test)  \n",
        "\n",
        "\n",
        "  def pick_n_images_per_digit(self, num_images_per_digit=1, train=True):\n",
        "    train_data , test_data = self.dataset\n",
        "    picked_numbers = [0]*10\n",
        "    picked_data = []\n",
        "    if train:\n",
        "      data_used = train_data\n",
        "    else:\n",
        "      data_used = test_data\n",
        "\n",
        "    data_used_zipped_shuffled = list(zip(data_used[0], data_used[1]))\n",
        "    np.random.shuffle(data_used_zipped_shuffled)\n",
        "    data_used_zipped_shuffled_x, data_used_zipped_shuffled_y = zip(*data_used_zipped_shuffled)\n",
        "\n",
        "    for x, y in zip(data_used_zipped_shuffled_x, data_used_zipped_shuffled_y):\n",
        "      if picked_numbers[y] < num_images_per_digit:\n",
        "        picked_data.append((x, y))\n",
        "        picked_numbers[y] += 1\n",
        "        if len(picked_data) >= num_images_per_digit * 10:\n",
        "          break\n",
        "    return picked_data\n",
        "\n",
        "\n",
        "  def put_picked_data_in_bins(self, picked_data, train=True):\n",
        "    binned_picked_data = [[] for _ in range(10)]\n",
        "    for x, y in picked_data:\n",
        "      binned_picked_data[y].append((x, y))\n",
        "    if train:\n",
        "      self.picked_data = binned_picked_data\n",
        "    else:\n",
        "      self.picked_data_test = binned_picked_data\n",
        "    return binned_picked_data\n",
        "\n",
        "\n",
        "  def flatten_data(self, binned_data, randomize=True):\n",
        "    flat_data = []\n",
        "    for bin in binned_data:\n",
        "      for datum in bin:\n",
        "        flat_data.append(datum)\n",
        "    if randomize is True:\n",
        "      np.random.shuffle(flat_data)\n",
        "    return flat_data\n",
        "\n",
        "\n",
        "  def test_with_one_network(self, learning_rate, num_epochs, network_type='conv'):\n",
        "    picked_training_data = self.pick_n_images_per_digit(num_images_per_digit=self.num_train_images_per_digit, train=True)\n",
        "    binned_picked_training_data = self.put_picked_data_in_bins(picked_data=picked_training_data, train=True)\n",
        "    \n",
        "\n",
        "    picked_test_data = self.pick_n_images_per_digit(num_images_per_digit=self.num_test_images_per_digit, train=False)\n",
        "    binned_picked_test_data = self.put_picked_data_in_bins(picked_data=picked_test_data, train=False)\n",
        "    \n",
        "\n",
        "    binned_picked_training_data = binned_picked_training_data[:self.num_classes]\n",
        "    binned_picked_test_data = binned_picked_test_data[:self.num_classes]\n",
        "\n",
        "    x_train = []\n",
        "    y_train = []\n",
        "\n",
        "    x_test = []\n",
        "    y_test = []\n",
        "\n",
        "    if network_type == 'conv':\n",
        "      classifier = self.conv_multi_classifier_model(learning_rate=learning_rate)\n",
        "    elif network_type == 'ff':\n",
        "      classifier = self.ff_multi_classifier_model(learning_rate=learning_rate)\n",
        "\n",
        "    for bin_index, training_bin in enumerate(binned_picked_training_data):\n",
        "        for training_datum in training_bin:\n",
        "          image, y_datum = training_datum\n",
        "          image = np.expand_dims(np.asarray(image), axis=2)\n",
        "          x_train.append(image)\n",
        "          y_train.append(y_datum)\n",
        "\n",
        "\n",
        "    for bin_index, test_bin in enumerate(binned_picked_test_data):\n",
        "        for test_datum in test_bin:\n",
        "          image, y_datum = test_datum\n",
        "          image = np.expand_dims(np.asarray(image), axis=2)\n",
        "          x_test.append(image)\n",
        "          y_test.append(y_datum)\n",
        "\n",
        "    print('Training The Traditional Network')\n",
        "    classifier.fit(np.asarray(x_train), np.asarray(y_train), batch_size=self.num_classes, epochs=num_epochs, verbose=0)\n",
        "    metrics = classifier.evaluate(np.asarray(x_test), np.asarray(y_test))\n",
        "    print('Accuracy With Traditional Network : %f' % metrics[1])\n",
        "    return metrics\n",
        "\n",
        "\n",
        "  def initialize_autoencoders(self):\n",
        "    for encoder_index, encoder_size in enumerate(self.encoder_sizes[:-1]):\n",
        "      autoencoder = self.shallow_autoencoder_model(encoder_size, encoder_index)\n",
        "      intermediate = tf.keras.Model(inputs=autoencoder.input, outputs=autoencoder.layers[2].output)\n",
        "      self.autoencoders.append(autoencoder)\n",
        "      self.intermediates.append(intermediate)\n",
        "\n",
        "  def train_autoencoders(self, train_data):\n",
        "    for _ in range(self.num_epochs):\n",
        "      np.random.shuffle(train_data)\n",
        "      for index, (x, y) in enumerate(train_data):\n",
        "        out = x\n",
        "        for index, autoencoder in enumerate(self.autoencoders):\n",
        "          autoencoder.fit(np.asarray([out]), np.asarray([out]), batch_size=1, epochs=1, verbose=0)\n",
        "          out = self.intermediates[index](np.asarray([out]))\n",
        "          out = np.squeeze(out, axis=0)\n",
        "          out = np.reshape(out, newshape=(int(math.sqrt(out.shape[0])), int(math.sqrt(out.shape[0]))))\n",
        "        out_flattened = np.reshape(out, newshape=(out.shape[0]*out.shape[0],))\n",
        "        self.outputs[y].append(out_flattened)\n",
        "        self.sums[y] = self.sums[y] + out_flattened\n",
        "        self.averages[y] = self.sums[y] / len(self.outputs[y])\n",
        "        if len(self.outputs[y]) > 1:\n",
        "          self.std_devs[y] = math.sqrt(((self.std_devs[y]*(len(self.outputs[y])-2))+(math.pow(distance.euclidean(out_flattened, self.averages[y]), 2))) / (len(self.outputs[y])-1))\n",
        "        else:\n",
        "          self.std_devs[y] = 0.0\n",
        "\n",
        "  def evaluate_on_test_data(self, test_data):\n",
        "      score = 0\n",
        "      results = [[] for _ in range(self.num_classes)]\n",
        "      sums = [[0.0]*self.encoder_sizes[-1] for _ in range(self.num_classes)]\n",
        "      averages = [[0.0]*self.encoder_sizes[-1] for _ in range(self.num_classes)]\n",
        "      for index, (x, y) in enumerate(test_data):\n",
        "        out = x\n",
        "        for intermediate in self.intermediates:\n",
        "          out = intermediate(np.asarray([out]))\n",
        "          out = np.squeeze(out, axis=0)\n",
        "          out = np.reshape(out, newshape=(int(math.sqrt(out.shape[0])), int(math.sqrt(out.shape[0]))))\n",
        "        out_flattened = np.reshape(out, newshape=(out.shape[0]*out.shape[0],))\n",
        "        # plt.matshow(out)\n",
        "        # plt.show()\n",
        "        #distances = list(map(lambda avg_vec_index, avg_vec: mean_squared_error(avg_vec, out_flattened), enumerate(self.averages)))\n",
        "        distances = [0.0 for _ in range(self.num_classes)]\n",
        "        for index, avg in enumerate(self.averages):\n",
        "          dist = distance.euclidean(avg, out_flattened)\n",
        "          z_value = abs(dist / self.std_devs[index])\n",
        "          distances[index] = z_value\n",
        "        distances = softmax(distances)\n",
        "        predicted_digit = np.argmin(distances)\n",
        "        if predicted_digit == y:\n",
        "          score += 1\n",
        "        # else:\n",
        "        #   print(\"predicted: {}, actual: {}\".format(predicted_digit, y))\n",
        "        #   print(distances)\n",
        "        results[y].append(out_flattened)\n",
        "        sums[y] = sums[y] + out_flattened\n",
        "        averages[y] = sums[y] / len(results[y])\n",
        "        \n",
        "\n",
        "      self.results = results      \n",
        "      print('Accuracy is {}'.format((score / len(test_data))))\n",
        "\n",
        "  def train_conv_model(self, train_data):\n",
        "    conv_encoder = self.conv_encoder()\n",
        "    #conv_encoder.summary()\n",
        "    x_train, y_train = zip(*train_data)\n",
        "    conv_encoder.fit(np.expand_dims(np.asarray(x_train), axis=-1), np.expand_dims(np.asarray(x_train), axis=-1), batch_size=1, epochs=1)\n",
        "    conv_intermediate = tf.keras.Model(inputs=conv_encoder.input, outputs=conv_encoder.layers[7].output)\n",
        "    return conv_intermediate\n",
        "  \n",
        "  def transform_train_and_test_data(self, conv_intermediate, train_data, test_data):\n",
        "    def transform_data_point(row):\n",
        "      x = conv_intermediate(np.asarray([row[0]]))\n",
        "      x = np.squeeze(x, axis=0)\n",
        "      x = np.reshape(x, newshape=(int(math.sqrt(x.shape[0])), int(math.sqrt(x.shape[0])), 1))\n",
        "      new_row = (x, row[1])\n",
        "      return new_row\n",
        "\n",
        "    transformed_train_data = list(map(transform_data_point, train_data))\n",
        "    transformed_test_data = list(map(transform_data_point, test_data))\n",
        "    \n",
        "    return transformed_train_data, transformed_test_data\n",
        "    \n",
        "\n",
        "  def plot_pca_reduced_points(self):\n",
        "    pca = PCA(n_components=2)\n",
        "    pca_input = []\n",
        "    for result in self.results:\n",
        "      for entry in result:\n",
        "        pca_input.append(entry)\n",
        "    pca_output = pca.fit(np.array(pca_input))\n",
        "    for result in self.results:\n",
        "      output = pca.transform(result)\n",
        "      x, y = zip(*output)\n",
        "      plt.scatter(x, y)\n",
        "    plt.show()\n",
        "  \n",
        "  def start(self):\n",
        "\n",
        "    picked_training_data = self.pick_n_images_per_digit(num_images_per_digit=self.num_train_images_per_digit, train=True)\n",
        "    binned_picked_training_data = self.put_picked_data_in_bins(picked_data=picked_training_data, train=True)\n",
        "    \n",
        "\n",
        "    picked_test_data = self.pick_n_images_per_digit(num_images_per_digit=self.num_test_images_per_digit, train=False)\n",
        "    binned_picked_test_data = self.put_picked_data_in_bins(picked_data=picked_test_data, train=False)\n",
        "\n",
        "    binned_picked_training_data = binned_picked_training_data[:self.num_classes]\n",
        "    binned_picked_test_data = binned_picked_test_data[:self.num_classes]\n",
        "\n",
        "    train_data = self.flatten_data(binned_picked_training_data, randomize=True)\n",
        "    test_data = self.flatten_data(binned_picked_test_data, randomize=True)\n",
        "    \n",
        "    ### Using A Pre-Conv Layer - Turn Off If Not Desired\n",
        "    # conv_intermediate = self.train_conv_model(train_data)\n",
        "    # train_data, test_data = self.transform_train_and_test_data(conv_intermediate, train_data, test_data)\n",
        "    ###\n",
        "    self.initialize_autoencoders()\n",
        "    self.train_autoencoders(train_data)\n",
        "    self.evaluate_on_test_data(test_data)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wLFTtWTqk8L"
      },
      "source": [
        "**Calls To Start The Experiment**\n",
        "\n",
        "Options for network_type: 'conv' or 'ff'\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwrUbP-oqrhh",
        "outputId": "64a05e31-8414-424b-f564-367113935f78"
      },
      "source": [
        "# Parameters of the experiment\n",
        "num_epochs = 1\n",
        "ipd = 20\n",
        "ipd_test = 100\n",
        "learning_rate_sp = 0.000001\n",
        "learning_rate_preconv = 0.0005\n",
        "learning_rate_trad = 0.001\n",
        "num_classes = 10\n",
        "encoder_sizes = [784, 196, 64]\n",
        "\n",
        "# Instantiate the Experiment Class\n",
        "experiment = Experiment(\n",
        "                  learning_rate=learning_rate_sp,\n",
        "                  learning_rate_trad=learning_rate_trad,\n",
        "                  learning_rate_preconv=learning_rate_preconv,\n",
        "                  num_classes=num_classes,\n",
        "                  num_train_images_per_digit=ipd,\n",
        "                  num_test_images_per_digit=ipd_test,\n",
        "                  num_epochs=num_epochs,\n",
        "                  encoder_sizes=encoder_sizes)\n",
        "#Start the experiment which trains the specialized networks and tests on them\n",
        "experiment.start()\n",
        "#experiment.plot_pca_reduced_points()\n",
        "#im = plt.matshow(experiment.autoencoders[0].layers[1].weights[0])\n",
        "#Test on a traditional conv net or ff net.\n",
        "#experiment.test_with_one_network(learning_rate_trad, num_epochs, network_type='ff')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy is 0.565\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}